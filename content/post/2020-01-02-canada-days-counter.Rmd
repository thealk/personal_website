---
title: "Counting Days in Canada: Using ggmap to track time spent in Canada"
author: Thea
date: '2020-01-02'
slug: canada-days-counter
categories:
  - ggplot
  - R
  - personal
tags:
  - ggmap
  - R
  - rstats
  - google maps
header:
  caption: ''
  image: ''
  preview: yes
draft: FALSE
---

```{r, echo = FALSE}
library(emo)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE, warning=FALSE, fig.align="center")
load('/Users/theaknowles/Documents/GitHub/personal_website/static/project/canada_days_counter.RData')
```

# TL;DR
I used my timeline from Google Maps along with the `ggmap` package to count the number of days I have spent in Canada in the last 4 years in order to facilitate my Canadian citizenship application.

`r emo::ji("warning")` **Disclaimer: n00b**: This is the first time I have dabbled with wrangling map data in R (outside of R-Ladies workshops). I am not a geocoder and this is no way relates to what I do for research (aside from wrangling data). In fact, anyone who has spent time with me travelling between any two points can probably attest to the fact that I have absolutely zero geospatial intuition whatsoever. Luckily, that doesn't matter here. What matters more is that there is a whole industry that works with geographical information systems, and I know virtually nothing about it. Therefore, this information, while functional for this task, is really devoid of any deep knowledge of GIS and the field at large. This is not meant to provide a forray into that arena, merely to demonstrate how I wrangled my own location data to achieve a small goal.

- [Skip to the code](#howto)

# The problem

<center>
<iframe src="https://giphy.com/embed/3FmmhJdHN4PSESllzZ" width="480" height="360" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/the-simpsons-canada-ralph-wiggum-3FmmhJdHN4PSESllzZ">via GIPHY</a></p>
</center>

I recently became eligible to apply for Canadian citizenship. I am a US citizen, but I have been living in Canada since 2008. I became a permanent resident of Canada in November 2015. I moved back to the US in August 2019. One of the eligibility requirements for applying for Canadian citizenship is, in addition to being a permanent resident for at least 3 years, that you "must have been physically present in Canada for at least 1095 days during the five years before you apply" ([Canada Immigration](https://www.canada.ca/en/immigration-refugees-citizenship/services/canadian-citizenship/become-canadian-citizen/eligibility.html#time)). You are required to list all the trips you took that brought you out of the country.
The Canadian immigration website suggests you use a "travel journal" to document your trips out of Canada.

My main challenge is that I travel in and out of Canada *all the time*. I am the only member of my family in Canada. When I lived in Montreal, I visited my parents and friends in MA regularly. We took frequent trips to Vermont (mainly to score [Heady Topper beer](https://alchemistbeer.com/)). When I moved to London, ON, I travelled even more, sometimes heading down to Detroit for just a day or two. My partner moved overseas in 2015, prompting several international trips during my PhD, not to mention conferences, family meet ups, and vacations.

Did I keep a travel journal in careful preparation of my eventual Canadian citizenship application? No, no I did not.

Having been a PR for just over 4 years, and now having no immediate plans to return to Canada, my window of eligibility for citizenship is closing. I need to determine 1) how many days I have spent in Canada so far to ensure I know when my eligibility will likely expire and 2) document my international trips, which I will need to report on my application.

My cries for help on social media were met with dire responses - there is no easy way around this step. Friends described culling over old emails, Google calendars, passport pages to determine exactly when they were in and out of Canada. You can request an official document of your travels, but this is country specific, takes awhile to process, and, from what I've heard, may not even be complete either. Given my frequent back and forths, I needed to figure out a better, faster, more reliable way to document my travels.

# The solution: Location tracking

<center>
<iframe src="https://giphy.com/embed/fR6Y3QxkEdTX2" width="480" height="417" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/television-jon-stewart-daily-show-fR6Y3QxkEdTX2">via GIPHY</a></p>
</center>

Every year, Google Maps sends me a cute and slightly creepy summary of all the places I've been - a haunting reminder that, since I never turn my location services off on my phone, which is always with me, Google knows exactly where I am at all times, how long I stay there, and how I travel there (estimated based on speed of transport between locations).

<center>
```{r eval = TRUE, echo = FALSE}
blogdown::shortcode('tweet', '1081209086014820352')
```
</center>

While that is a bit jarring and worthy of its own ethical debate, in my case for the above problem, it turned out to be EXTREMELY handy.

Google maps allows you to download your own timeline data in the form of a json file. The [`ggmap`](https://github.com/dkahle/ggmap) package allows you to parse and plot location data[^1135]. In the end, I was able to read in all of my timeline data since before I became a permanent resident[^552], and use it to 1) figure out how many days I was in Canada, and 2) have a fairly accurate record of the trips I took outside of Canada without having to rely on other sketchy, unreliable forms of documentation (or, god forbid, my MEMORY).

[^1135]: Rebecca Henderson gave a workshop last year for our R-Ladies group on `ggmap` and plotting local bike collision data in R. Her presentation and materials were super helpful. See her materials [here](https://github.com/rladies/meetup-presentations_london_ontario/tree/master/2019-03-28_ggmap).

[^552]: Technically if you have been a Canadian PR for less than five years but were living in Canada, you are also allowed to count those days as well. Days spent in Canada before you became a PR within the 5 year time frame count as 0.5 days a piece, up to 365 days ([Candian Immigration](https://www.canada.ca/en/immigration-refugees-citizenship/services/canadian-citizenship/become-canadian-citizen/eligibility.html#time)). In my case, I'm not worrying too much about these days at this point since I have enough days since becoming a PR.

# Steps {#howto}

While looking into possibilities for wrangling my Maps data, I came across [this blog post](https://shiring.github.io/maps/2016/12/30/Standortverlauf_post), which gives a nice step-by-step of how to do download your own location data and parse it to look for patterns. I've summarized some of those initial steps here, and then used the tidied data to further wrangle for my trip counter.

## 1. Load libraries

```{r eval = TRUE}
library(jsonlite)
library(ggmap)
library(dplyr)
library(stringr)
library(tidyr)
```

## 2. Get a Google API console key

You need this to be able to use the geocoding functions in ggmap, as well as to be able to plot. I learned how to do this from Rebecca Henderson's R-Ladies workshop, in which she pointed us to this [Medium article](https://medium.com/superkoders/ultimate-guide-to-google-maps-d86ad945636a) explaining how and why you need an API. Note that you need to enable billing (i.e., with a credit card), but don't need to pay to use the service.

Once you have successfully obtained an API console key, you need to enable it in your R session:

```{r eval = FALSE}
ggmap::register_google("your-secret-api-key")
```

## 2. Download your location history from your Google Account

You can download your own Google data, including from Maps, via [Google takeout](https://takeout.google.com/settings/takeout). Be sure to only select "Location History." This can take a little while (it took about 30 minutes for me to download the last 4 years, which is my entire Maps timeline). The final output is a json file called `Location History.json`. You can also do this from within [Google Maps](https://www.google.com/maps) by going to "Your Data in Maps" >> "Download your Maps Data."

## 3. Parse the location history file

The [`jsonlite`](https://cran.r-project.org/web/packages/jsonlite/index.html) R package allows you to read in json files into R for further manipulation. I moved my .json output file to my R project folder and ran the following. This took a couple minutes.

```{r eval=FALSE}
file <- "Location History.json"
lh <- read_json(file, simplifyVector = TRUE)
```


Next you need to extract the location data and convert the time and date into a human-readable format. The output contains more columns of information (including velcocity, activity, and estimated accuracy), which I've excluded in the preview.

```{r eval=TRUE}
locations <- lh$locations
locations$time = as.POSIXct(as.numeric(locations$timestampMs)/1000, 
                            origin = "1970-01-01")

locations %>% select(1:3,time) %>% head()
```

You can check your earliest and most recent maps data time stamps using `min()` and `max()` functions. 
```{r eval = TRUE}
min(locations$time)
max(locations$time)
```

My location tracking has been on since July 2015. The most recent point reflects the last time I downloaded my Maps data.

## 4. Tidy the location history
Latitude and longitude are recorded in Google in E7 format. You need to divide the original longitude/latitude values by 10^7 to get standard coordinate formatting.

I also found it helpful to round my final coordinates. The more decimal places you include, the more precise your location identification will be. In my case, all I really cared about was whether I could reliably capture border crossings. 
Some [light Wikipediaing](https://en.wikipedia.org/wiki/Decimal_degrees) informed me that rounding coordinates to the tens gives you a resolution of 11.1 km, roughly allowing you to distinguish major cities. Zero decimal places, i.e., rounding to the nearest integer, allows for 111.1 km precision, which can unambiguously capture large regions and countries (see also [here](https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude)). Rounding *greatly* reduces the amount of data you have to work with, which speeds processing up tremendously later on. It's also simply not *necessary*, IMHO, at least for this project, to know your location down to the meter. 

To prepare for further reduction, I also added a coordinates column (`coord`).

```{r eval = FALSE}
loc <- loc %>% mutate(
     date = str_sub(time,1,10),
     time = str_sub(time,12,19),
     lon = round((loc$longitudeE7 / 1e7), 0),
     lat = round((loc$latitudeE7 / 1e7), 0),
     coord = paste(lon,lat,sep="_")
) 
```

I became a PR on November 22, 2015. I've filtered out dates prior to this (however, days within the 5 year period prior to obtaining PR status *do* count as half days, but for now I'm going to omit them).

```{r}
loc <- locations %>% filter(date >= "2015-11-22")
```


I grouped the location data by date and coordinates - that is, one row for each unique date-coordinate combination. Since I had rounded my coordinates to 0 decimals earlier, this now gave me one row for approximately every 111 km. Importantly, I was able to collapse over days that I drove around a single city or over shorter distances. This allowed me to speed up further parsing below.

```{r}
loc <- loc %>% 
  group_by(date,coord) %>% 
  summarize(n=n()) %>% 
  separate(lonlat, c("lon","lat"), sep="_", remove=FALSE) %>%
  mutate(lon = as.numeric(lon),
         lat = as.numeric(lat)) %>%
  ungroup()
```

To test whether rounding to the nearest integer was sufficient, I made a temporary data frame of 1 day's worth of location history on a date that I know I crossed from Ontario to New York. This allowed me to confirm that rounding to 0 decimal places was more or less enough to capture what I wanted, knowing that this may mean I don't capture every single day with 100% accuracy (for example, this runs the risk of days that I spent *near* a border being counted as *crossing* a border - a risk I felt was worth the time saved by the data reduction). 
You could just as easily a month or other specified time span.

```{r eval = TRUE}
tmp_day <- loc %>% 
  filter(date == "2019-10-20") # Look just at Oct 20 2019
  
tmp_day %>% select(1:6)
```

The n column reflects how many data points were logged for that coordinate on that day.

## 5. Coordinates `r emo::ji("point_right")` Country: Reverse geocoding

*Geocoding* allows you to convert addresses into geographic coordinates.
*Reverse geocoding* allows you to do the opposite: convert coordinates to addresses. 
The `ggmap` package provides a function `revgeocode()` to do just this. 

I created a small function to use `revgeocode()` to get the address from my location data. I used the `stringr` package to extract the country I was in for each observation - the country of the address is always(?) the last element of the list produced by `revgeocode()`. This function returns either just the country, or the full address retrieved from Google.

```{r echo=TRUE}
get_country <- function(lon,lat, out=c("country","address")){
  address <- suppressMessages(
    revgeocode(
      as.vector(
        c(as.numeric(lon),
          as.numeric(lat)))))
  
  # Split address by commas
  address_split <- str_split(address, pattern = ", ")
  # Country is always the last element
  l <- length(address_split[[1]]) 
  country <- address_split[[1]][l]
  
  # Convert NAs to characters to make life easier later
  if(is.na(country)){country <- "NA"}
  if(is.na(address)){address <- "NA"}
    
  if(out=="country"){
    return(country)
  }else if(out=="address"){
    return(address)
  }
}
```

I then created another function to determine, for a given data point, whether I was or was not in a given country (e.g., Canada). This function returns 1 if "country" equals the string "Canada", and 0 if not. This allows me to eventually sum the days spent in Canada.

```{r}
visited_country <- function(x, country = "Canada"){
  if(x==country){
    visit <- 1
  }else{visit <- 0}
}
```

Next I used `mapply()` to iterate over each observation in my location history data, applying my `get_country()` and `visited_country()` functions to each row. 

```{r}
# Get country
loc$country <- mapply(get_country, 
                      loc$lon,loc$lat, 
                      out = "country")
# Get address
loc$address <- mapply(get_country,
                      loc$lon,loc$lat,
                      out = "address")

# Log whether I visited Canada
loc$visited <- mapply(visited_country, 
                      loc$countries, 
                      "Canada")

head(loc) # roads were renamed for privacy
```

```{r echo=FALSE, eval=TRUE}
# Rename roads for privacy
loc$address <- str_replace_all(loc$address, (c("Hunt" = "Swan", "Myrtle" = "Martin", "McCarthy" = "McCormick", "Adrian" = "Adler")))

subset(loc, date > "2015-11-30") %>% head()
```

Note that I modfied the street names so as not to show actual addresses in this post. Because of the coordinate rounding, the addresses also likely do not reflect my exact location (just a location within 111km).


Finally, I computed how many days I was in Canada by grouping my data by date and summing my visits.
```{r}
days_in_country <- loc %>%
  group_by(date) %>%
  summarize(n_in_canada = sum(visited),
            countries = paste(country,collapse=", ")) %>%
  ungroup() %>%
  mutate(in_canada = ifelse(n_in_canada > 0,1,0))
```

```{r echo=FALSE, eval=TRUE}
# convert for post
days_in_country <- days_in_canada
```


```{r eval=TRUE}
sum(days_in_country$in_canada)
```

Looks like, since November 22, 2015, I've spent 1101 days in Canada, *just* over the required 1095! Since my pre-PR days count for some to use as padding (and since I do still go back and forth to Canada periodically), this is good news. I'll be eligible to apply for another year. 


## 6. Logging Trips
I could also now collapse over countries and days for a log of the trips that I took. I only care about trips outside of Canada, so I'll filter Canada out first, then set up columns for trip "start" and "end" dates. 

```{r eval=TRUE}
trips <- loc %>%
  filter(country!="Canada")%>%
  group_by(date, country) %>%
  summarize(start = min(date),
            end = max(date)) %>%
  ungroup()

head(trips)
```

Next, I "rolled up" the dates of each trip by using the `lag()` function (thank you to this [helpful StackOverflow post](https://stackoverflow.com/questions/37486572/date-roll-up-in-r/37487673#37487673) because I had no clue what I was doing here). I imagine the [`lubridate` package](https://lubridate.tidyverse.org/) would also be super handy here, but I didn't go down that route.

```{r eval=TRUE}
trips <- trips %>%
  mutate(gr = cumsum(start-lag(end, default=1) != 1)) %>%
  group_by(gr, country) %>%
  summarise(FromDate = min(start), 
            ToDate   = max(end),
            dur = as.Date(ToDate) - as.Date(FromDate)) %>%
  ungroup()
```

```{r eval=TRUE, echo=FALSE}
trips$country <- plyr::revalue(trips$country, c("NA" = "Israel"))
```


This glimpse of the earliest part of my trip log captures a couple short trips to the US, a trip to Isreal, a layover in Belgium, and a return to the US.

```{r eval=TRUE}
head(trips,5)
```



# Some odd findings
While for the most part, these points appear to be confirmed by memory and calendar notes, a few oddities arose. According to my maps data, I visited the following countries:

```{r eval=TRUE}
xtabs(~country, data=loc)
```


<!-- One easy-to-fix issue is that, for whatever reason, the US is coded most of the time as "USA" and once as "United States". For my purposes, this doesn't matter, but it probably makes sense to collapse these two: -->

```{r include=FALSE}
loc <- loc %>%
  mutate(country = ifelse(country=="United States","USA",country))
```


This breakdown isn't 100% accurate. I have never actually visited Thailand or Indonesia. The high number of NAs are a bit worrisome too. 

Closer inspection reveals that most, but not all, of these blips occured on days I was travelling to or around Israel (where my partner was a post-doc for 2 years). I recall Maps having a hard time localizing us sometimes, so perhaps there is something to do with Google's satellite signal or access in this region. To be fair, the Google Maps timeline interface doesn't show these blips, so whatever's happening is getting parsed out by Google Map's final algorithms.

At the end of the day, all I care about is whether it accurately logs whether I was in Canada (even for just a single time point) on a given day. This does seem to be accurate. Even if misses a few days (e.g., sometimes my phone registers me as being in the US when I am at the a Canadian border city, and vice versa), that's still accurate enough for me at this stage.


# Plotting (optional)
As much as I wish I'd get bonus points from the Canadian government for being able to provide a dataviz of my time in the country, this is sadly just not how offical governmental procedures work, apparently. Practically speaking, then, plotting my maps is just a little additional dopamine rush for me. I *did* want to be able to visually confirm that my coordinates made sense while getting this set up, though, so I've included some basic code for how to do that here. Be it known that there are many other blogs that have much more interesting dataviz tutorials that harness `ggmap` and Google Map timelines, however.

Since I was mostly interested in my presence in Canada in the last five years, during which time I've mostly been in Ontario, we'll plot just that region for now. The `get_map()` function in `ggmap` allows you to download maps of various locations. Deciding the appropriate "zoom" level takes a bit of trial and error and depends on your goals. In this case, 5 works well for seeing the province of Ontario and some surrounding area.

The following maps clearly show the difference between different degrees of coordinate rounding precision.

```{r eval=FALSE}
on <- ggmap::get_map("Ontario", zoom=5)

library(ggplot2)
# Get map for southwestern Ontario
swon <- get_map("Toronto", zoom=5)

ggmap(swon) + 
  geom_point(data = loc, 
             aes(x = lon, y = lat), 
             alpha = 0.5, color = "red")
```

```{r eval=TRUE, echo=FALSE}
map_0_decimals
```


Here's the same map but with coordinates rounded to 2 decimal places instead, for a resolution of 1.11 km (i.e., unambiguously distinguish towns/villages. Big difference!

```{r echo = FALSE, eval=TRUE}
map_2_decimals
```

Guess now I shoud finally get to the actual application...

# Resources that came in handy
- [Shirin Glander's](https://twitter.com/ShirinGlander) [blog post](https://shiring.github.io/maps/2016/12/30/Standortverlauf_post)
- [Rebecca Henderson's R-Ladies `ggmap` presentation](https://github.com/rladies/meetup-presentations_london_ontario/tree/master/2019-03-28_ggmap)
- The [`ggmap`](https://github.com/dkahle/ggmap) package
- [Google takeout](https://takeout.google.com/settings/takeout)
- [More on geocoding](http://www.storybench.org/geocode-csv-addresses-r/)






